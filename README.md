# RAG Query Classifier

A fast, domain-agnostic query classification system that determines whether user queries are **relevant**, **irrelevant**, or **vague**. Perfect for RAG (Retrieval-Augmented Generation) systems to filter and route queries appropriately.

## üöÄ Features

- **‚ö° Fast Classification**: Multi-level classification pipeline with caching
- **üåê Domain Agnostic**: Works for any domain (business, medical, legal, etc.)
- **üîß Configurable**: Easy domain-specific customization via YAML config
- **ü§ñ LLM Integration**: Uses local LLMs via Ollama for complex classification
- **üìä Structured Output**: Returns confidence scores and reasoning
- **üíæ Smart Caching**: LRU cache for instant repeated query responses
- **‚öôÔ∏è Rule-Based Fallback**: Fast pattern matching before LLM calls

## üìã Requirements

- Python 3.8+
- [Ollama](https://ollama.ai/) with a compatible model (e.g., `phi3:mini`, `llama2:7b`, `mistral:7b`)

## üõ†Ô∏è Installation

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd RAG-query-classifier
   ```

2. **Create and activate a virtual environment:**
   ```bash
   # Create virtual environment
   python -m venv venv
   
   # Activate virtual environment
   # On macOS/Linux:
   source venv/bin/activate
   # On Windows:
   venv\Scripts\activate
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install and start Ollama:**
   ```bash
   # Install Ollama (macOS/Linux)
   curl -fsSL https://ollama.ai/install.sh | sh
   
   # Pull a model (recommended: phi3:mini)
   ollama pull phi3:mini
   ```

5. **Start Ollama server:**
   ```bash
   ollama serve
   ```

### Virtual Environment Management

**To deactivate the virtual environment:**
```bash
deactivate
```

**To reactivate the virtual environment later:**
```bash
# On macOS/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate
```

**To remove the virtual environment:**
```bash
rm -rf venv
```

## ‚öôÔ∏è Configuration

The classifier is configured via `config.yaml`. Here's the structure:

```yaml
rules:
  relevant_keywords:
    - "account"
    - "invoice"
    - "payment"
    - "order"
    - "subscription"
    - "reset password"
    - "password"
  irrelevant_patterns:
    - "weather"
    - "sports"
    - "joke"
    - "news"
    - "movie"
  vague_patterns:
    - "help"
    - "question"
    - "info"
    - "support"
    - "assist"

examples:
  relevant:
    - "How do I reset my account password?"
    - "What is the status of my last order?"
    - "Can I get a copy of my invoice?"
  irrelevant:
    - "Tell me a joke."
    - "What's the weather today?"
    - "Who won the football game?"
  vague:
    - "I need help."
    - "Can you assist me?"
    - "Support."
```

### Configuration Options

- **`relevant_keywords`**: Words/phrases that indicate relevant queries
- **`irrelevant_patterns`**: Patterns that indicate irrelevant queries
- **`vague_patterns`**: Patterns that indicate vague/unclear queries
- **`examples`**: Training examples for the LLM classifier

## üöÄ Quick Start

### Basic Usage

```python
from rag_query_classifier import QueryClassifier

# Initialize classifier
classifier = QueryClassifier("config.yaml")

# Classify a query
result = classifier.classify("How do I reset my password?")

print(f"Label: {result.label.value}")
print(f"Confidence: {result.confidence_score}")
print(f"Reasoning: {result.reasoning}")
print(f"Source: {result.source}")
print(f"Action: {classifier.action(result)}")
```

### Performance Testing

```python
import time
from rag_query_classifier import QueryClassifier

classifier = QueryClassifier("config.yaml")

queries = [
    "How do I change my password?",
    "What's the weather like?",
    "I need help with my account",
    "Tell me a joke",
    "asdfgh"  # gibberish
]

for query in queries:
    start_time = time.time()
    result = classifier.classify(query)
    end_time = time.time()
    
    print(f"Query: {query}")
    print(f"  Classification: {result.label.value}")
    print(f"  Confidence: {result.confidence_score:.3f}")
    print(f"  Time: {(end_time - start_time) * 1000:.2f} ms")
    print(f"  Source: {result.source}")
    print()
```

## üìä Classification Results

The classifier returns a `ClassificationResult` object with:

- **`label`**: `QueryLabel.RELEVANT`, `QueryLabel.IRRELEVANT`, or `QueryLabel.VAGUE`
- **`confidence_score`**: Float between 0.0 and 1.0
- **`reasoning`**: String explanation of the classification
- **`source`**: Classification method used (`rule`, `llm (phi3:mini)`, `cache`, etc.)

## üéØ Classification Logic

The classifier uses a multi-level approach:

1. **Cache Check** (1ms): Returns cached results instantly
2. **Quick Patterns** (1-5ms): Detects very short queries
3. **Rule-Based** (5-10ms): Matches config patterns
4. **LLM Classification** (1000-3000ms): Complex reasoning with local LLM

### Classification Criteria

- **Relevant**: Clear, specific questions related to your domain
- **Irrelevant**: Entertainment, weather, sports, etc.
- **Vague**: Too short, unclear, or contains non-English text

## üîß Advanced Configuration

### Using Different Models

```python
# Fast model (less accurate)
classifier = QueryClassifier("config.yaml", model="tinyllama:latest")

# Balanced model (default)
classifier = QueryClassifier("config.yaml", model="phi3:mini")

# Accurate model (slower)
classifier = QueryClassifier("config.yaml", model="llama2:7b")
```

### Custom Endpoint

```python
from rag_query_classifier import LLMClassifier

# Use custom Ollama endpoint
llm = LLMClassifier(
    endpoint="http://localhost:11434/api/generate",
    model="phi3:mini"
)
```

### Cache Configuration

```python
# Increase cache size
llm = LLMClassifier(cache_size=5000)

# Disable caching
llm = LLMClassifier(cache_size=0)
```

## üèóÔ∏è Architecture

```
Query Input
    ‚Üì
Cache Check ‚Üí [Cached?] ‚Üí Return Result
    ‚Üì No
Quick Patterns ‚Üí [Match?] ‚Üí Return Result
    ‚Üì No
Rule-Based ‚Üí [Match?] ‚Üí Return Result
    ‚Üì No
LLM Classification ‚Üí Return Result
```

## üìà Performance

Typical performance on a modern machine:

- **Cached queries**: ~1ms
- **Pattern matches**: ~1-5ms
- **Rule matches**: ~5-10ms
- **LLM calls**: ~1000-3000ms (first time)
- **Subsequent LLM calls**: ~500-1500ms (cached prompts)

## üîç Troubleshooting

### Common Issues

1. **"Connection refused" error**
   - Ensure Ollama is running: `ollama serve`
   - Check if the endpoint is correct

2. **Slow performance**
   - Use a faster model like `tinyllama:latest`
   - Increase cache size
   - Add more rule-based patterns

3. **Incorrect classifications**
   - Update `config.yaml` with domain-specific rules
   - Add more examples to the config
   - Use a more accurate model

4. **JSON parsing errors**
   - The LLM might be returning malformed JSON
   - Try a different model
   - Check the system prompt

### Debug Mode

```python
# Enable debug output
import logging
logging.basicConfig(level=logging.DEBUG)
```

## üß™ Testing

Run the example script:

```bash
python example_usage.py
```

This will test various query types and show performance metrics.

## ü§ù Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Add tests
5. Submit a pull request

## üìÑ License

[Add your license here]

## üôè Acknowledgments

- Built with [Ollama](https://ollama.ai/) for local LLM inference
- Inspired by RAG system query filtering needs
